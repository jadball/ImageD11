{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8deabe5b",
   "metadata": {},
   "source": [
    "# Jupyter notebook based on ImageD11 to process scanning 3DXRD data\n",
    "# Written by Haixing Fang, Jon Wright and James Ball\n",
    "## Date: 26/02/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc9a47-acd9-4174-9aef-a7c27e7534bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NOTE: These notebooks are under active development\n",
    "They require the latest version of ImageD11 from Git to run.\n",
    "\n",
    "If you don't have this set up yet, you can run the below cell.\n",
    "\n",
    "It will automatically download and install ImageD11 to your home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3f4e7-08a2-4826-b098-c941a667642e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "username = os.environ.get(\"USER\")\n",
    "\n",
    "# USER: You can change this location if you want\n",
    "\n",
    "id11_code_path = f\"/home/esrf/{username}/Code/ImageD11\"\n",
    "\n",
    "# check whether we already have ImageD11 here\n",
    "\n",
    "if os.path.exists(id11_code_path):\n",
    "    raise FileExistsError(\"ImageD11 already present! Giving up\")\n",
    "\n",
    "!git clone https://github.com/FABLE-3DXRD/ImageD11 {id11_code_path}\n",
    "output = !cd {id11_code_path} && python setup.py build_ext --inplace\n",
    "\n",
    "if not os.path.exists(os.path.join(id11_code_path, \"build\")):\n",
    "    raise FileNotFoundError(f\"Can't find build folder in {id11_code_path}, compilation went wrong somewhere\")\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, id11_code_path)\n",
    "\n",
    "# if this works, we installed ImageD11 properly!\n",
    "try:\n",
    "    import ImageD11.cImageD11\n",
    "except:\n",
    "    raise FileNotFoundError(\"Couldn't import cImageD11, there's a problem with your Git install!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fcab9-7631-439f-885c-438bcefeac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "username = os.environ.get(\"USER\")\n",
    "\n",
    "# USER: Change the path below to point to your local copy of ImageD11:\n",
    "\n",
    "id11_code_path = f\"/home/esrf/{username}/Code/ImageD11\"\n",
    "\n",
    "if not os.path.exists(id11_code_path):\n",
    "    raise FileNotFoundError(\"Can't find Git version of ImageD11 in {id11_code_path}\")\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, id11_code_path)\n",
    "\n",
    "try:\n",
    "    import ImageD11.cImageD11\n",
    "except:\n",
    "    raise FileNotFoundError(\"Couldn't import cImageD11, there's a problem with your Git install!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c1db6-5a32-4294-abef-cfc2150d24de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import functions we need\n",
    "\n",
    "import glob, pprint\n",
    "import fabio\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import ImageD11.sinograms.dataset\n",
    "import ImageD11.sinograms.lima_segmenter\n",
    "import ImageD11.sinograms.assemble_label\n",
    "import ImageD11.sinograms.properties\n",
    "import ImageD11.nbGui.nb_utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import fabio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from skimage import filters, measure, morphology\n",
    "from ipywidgets import interact, interactive, widgets, fixed, Layout\n",
    "import h5py\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e3647-2e1b-4a31-b5de-01adbd4d7573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check that we're importing ImageD11 from the home directory rather than from the Jupyter kernel\n",
    "\n",
    "?ImageD11.sinograms.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb520f-cb00-4eeb-b740-fc7b407f1f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: For old datasets before the new directory layout structure, we don't distinguish between RAW_DATA and PROCESSED_DATA\n",
    "\n",
    "### USER: specify your experimental directory\n",
    "\n",
    "rawdata_path = \"/home/esrf/james1997a/Data/ihma439/id11/20231211/RAW_DATA\"\n",
    "\n",
    "!ls -lrt {rawdata_path}\n",
    "\n",
    "### USER: specify where you want your processed data to go\n",
    "\n",
    "processed_data_root_dir = \"/home/esrf/james1997a/Data/ihma439/id11/20231211/PROCESSED_DATA/James/20240226\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187950bd-18b5-4bd4-80da-2a0c7a984b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: pick a sample and a dataset you want to segment\n",
    "\n",
    "sample = \"FeAu_0p5_tR_nscope\"\n",
    "dataset = \"top_100um\"\n",
    "\n",
    "# USER: specify path to detector mask\n",
    "\n",
    "# mask_path = '/data/id11/inhouse1/ewoks/detectors/files/eiger_E-08-0173/mask_with_gaps_E-08-0173.edf'  # temporary eiger mask (Nov 2023)\n",
    "# mask_path = '/data/id11/inhouse1/ewoks/detectors/files/eiger_E-08-0144/mask.edf'  # normal eiger mask\n",
    "\n",
    "mask_path = '/data/id11/inhouse1/ewoks/detectors/files/eiger_E-08-0173/mask_with_gaps_E-08-0173.edf'  # temporary eiger mask (Nov 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad077c4b-39cc-4b90-9637-33c32f12e364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create ImageD11 dataset object\n",
    "\n",
    "ds = ImageD11.sinograms.dataset.DataSet(dataroot=rawdata_path,\n",
    "                                        analysisroot=processed_data_root_dir,\n",
    "                                        sample=sample,\n",
    "                                        dset=dataset)\n",
    "ds.import_all()\n",
    "ds.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c52da1-1948-4de7-b80b-d735bf8e8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: autodetect eiger/frelon (get from ds object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20a5bf-b3b9-489a-81d3-4b8e6c792d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the initial parameters\n",
    "start_pars = {#\"bgfile\": bg_path,\n",
    "              \"maskfile\": mask_path,\n",
    "              \"cut\": 1,\n",
    "              \"pixels_in_spot\": 3}\n",
    "\n",
    "mask = fabio.open(start_pars[\"maskfile\"]).data\n",
    "# convert to boolean\n",
    "mask = mask.astype(bool)\n",
    "\n",
    "# check if mask is mostly zeros or mostly ones\n",
    "mask_ones_fraction = np.sum(mask.ravel())/mask.ravel().shape[0]\n",
    "if mask_ones_fraction > 0.5:\n",
    "    print(\"Inverting mask!\")\n",
    "    # mask is mostly ones, we should invert it\n",
    "    mask = ~mask\n",
    "\n",
    "# import the image\n",
    "with h5py.File(ds.masterfile, 'r') as h5In:\n",
    "    # get middle scan\n",
    "    sorted_keys = sorted(h5In.keys(), key=lambda x: float(x))\n",
    "    middle_scan = sorted_keys[len(sorted_keys)//2]\n",
    "    single_scan_shape = h5In[f'{middle_scan}/measurement/eiger'].shape[0]\n",
    "    detector_frame = h5In[f'{middle_scan}/measurement/eiger'][single_scan_shape//2].astype('uint16')\n",
    "\n",
    "#bgimage = fabio.open(start_pars[\"bgfile\"]).data\n",
    "# no background\n",
    "bgimage = np.zeros_like(detector_frame)\n",
    "\n",
    "def segment_image(image, cut, pixels_in_spot, bgimage):\n",
    "    bgsub = image - bgimage\n",
    "    image_masked = image * ~mask\n",
    "    bgsub_masked = bgsub * ~mask\n",
    "    cut_image = bgsub_masked > cut\n",
    "    labeled_image = measure.label(cut_image)\n",
    "    regions = measure.regionprops(labeled_image)\n",
    "    blob_mask = np.zeros_like(image, dtype=bool)\n",
    "    spot_count = 0\n",
    "    for region in regions:\n",
    "        if region.area >= pixels_in_spot:\n",
    "            blob_mask[labeled_image == region.label] = 1\n",
    "            spot_count += 1\n",
    "    filtered_image = image_masked * blob_mask\n",
    "    return filtered_image, spot_count\n",
    "\n",
    "\n",
    "cut_slider = widgets.IntSlider(value=start_pars[\"cut\"], min=1, max=20, step=1, description='Cut:')\n",
    "pixels_in_spot_slider = widgets.IntSlider(value=start_pars[\"pixels_in_spot\"], min=1, max=20, step=1, description='Pixels in Spot:')\n",
    "\n",
    "# Display the image initially\n",
    "plt.figure()\n",
    "\n",
    "filtered_image, nspots = segment_image(detector_frame, cut=cut_slider.value, pixels_in_spot=pixels_in_spot_slider.value, bgimage=bgimage)\n",
    "im = plt.imshow(filtered_image, cmap=\"viridis\", norm=LogNorm(vmin=1, vmax=1000), interpolation=\"nearest\")\n",
    "plt.title(f\"Scan {middle_scan}, frame {single_scan_shape//2}\\n cut={cut_slider.value}, pixels_in_spot={pixels_in_spot_slider.value}, nspots={nspots}\")\n",
    "plt.show()\n",
    "\n",
    "def update_image(cut, pixels_in_spot):\n",
    "    filtered_image, nspots = segment_image(detector_frame, cut, pixels_in_spot, bgimage)\n",
    "    im.set_data(filtered_image)\n",
    "    plt.title(f\"Scan {middle_scan}, frame {single_scan_shape//2}\\n cut={cut}, pixels_in_spot={pixels_in_spot}, nspots={nspots}\")\n",
    "    plt.draw()\n",
    "\n",
    "interactive_plot = widgets.interactive(update_image, cut=cut_slider, pixels_in_spot=pixels_in_spot_slider)\n",
    "\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9c9ec-5301-4bf0-a7ba-2957afcc4db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_pars = {# \"bgfile\": bg_path,\n",
    "              \"maskfile\": mask_path,\n",
    "              \"cut\": cut_slider.value,\n",
    "              \"pixels_in_spot\": pixels_in_spot_slider.value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81929009-7445-434a-b19e-57bcd9e3e6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create batch file to send to SLURM cluster\n",
    "\n",
    "sbat = ImageD11.sinograms.lima_segmenter.setup(ds.dsfile, **end_pars)\n",
    "if sbat is None:\n",
    "    raise ValueError(\"This scan has already been segmented!\")\n",
    "print(sbat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827f616-bfae-45c1-969c-24da93886a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.slurm_submit_and_wait(sbat, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de23264-4c22-43fe-8be6-8cc54039ea2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# label sparse peaks\n",
    "\n",
    "ImageD11.sinograms.assemble_label.main(ds.dsfile, ds.sparsefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb029cd-7f66-4c91-abee-5f1723303360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate peaks table\n",
    "\n",
    "ImageD11.sinograms.properties.main(ds.dsfile, ds.sparsefile, ds.pksfile, options={'algorithm': 'lmlabel', 'wtmax': 70000, 'save_overlaps': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d215f-0ec9-4014-a49f-76e7c8167cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a new subfolder called \"sparse\" that holds all the individual \"scan______sparse.h5\" files\n",
    "\n",
    "sparse_folder_path = os.path.join(ds.analysispath, \"sparse\")\n",
    "\n",
    "if not os.path.exists(sparse_folder_path):\n",
    "    os.mkdir(sparse_folder_path)\n",
    "    \n",
    "scan_sparse_files = glob.glob(os.path.join(ds.analysispath, \"scan*_sparse.h5\"))\n",
    "\n",
    "for scan_sparse_file in scan_sparse_files:\n",
    "    shutil.move(scan_sparse_file, sparse_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e6575-bb8e-47ac-88e1-0ff5350ab1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: incorporate DATA/visitor/ma5839/id11/20240118/SCRIPTS/0_S3DXRD_segment_and_label_single_dset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d22d0-ef82-4e08-8087-c57e76e16de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 1:\n",
    "    raise ValueError(\"Change the 1 above to 0 to allow 'Run all cells' in the notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f70bb5-035b-48b2-9acd-39c6e3ea8666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we're happy with our segmentation parameters, we can run the below cell to do this in bulk for many samples/datasets\n",
    "# by default this will do all samples in sample_list, all datasets with a prefix of dset_prefix\n",
    "# you can add samples and datasets to skip\n",
    "\n",
    "skips_dict = {\n",
    "    \"FeAu_0p5_tR_nscope\": [\"top_-50um\", \"top_-100um\"]\n",
    "}\n",
    "\n",
    "dset_prefix = \"top\"\n",
    "\n",
    "sample_list = [\"FeAu_0p5_tR_nscope\"]\n",
    "    \n",
    "samples_dict = utils.find_datasets_to_process(rawdata_path, skips_dict, dset_prefix, sample_list)\n",
    "    \n",
    "# manual override:\n",
    "# samples_dict = {\"FeAu_0p5_tR_nscope\": [\"top_100um\", \"top_200um\"]}\n",
    "    \n",
    "# now we have our samples_dict, we can process our data:\n",
    "mask_path = '/data/id11/nanoscope/Eiger/eiger_mask_E-08-0173_20231127.edf'\n",
    "\n",
    "# you can change these if needed, but they will default to those you selected with the widget\n",
    "seg_pars = {\"maskfile\": mask_path,\n",
    "            \"cut\": cut_slider.value,\n",
    "            \"pixels_in_spot\": pixels_in_spot_slider.value}\n",
    "\n",
    "for sample, datasets in samples_dict.items():\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing dataset {dataset} in sample {sample}\")\n",
    "        dset_path = os.path.join(processed_data_root_dir, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")\n",
    "        \n",
    "        ds = ImageD11.sinograms.dataset.DataSet(dataroot=rawdata_path,\n",
    "                                                analysisroot=processed_data_root_dir,\n",
    "                                                sample=sample,\n",
    "                                                dset=dataset)\n",
    "        if os.path.exists(ds.sparsefile):\n",
    "            print(f\"Found existing Sparse file for {dataset} in sample {sample}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(\"Importing DataSet object\")\n",
    "        try:\n",
    "            ds.import_all()\n",
    "        except ValueError:\n",
    "            print(f\"Very dodgy scan! Skipping\")\n",
    "            continue\n",
    "        except KeyError:\n",
    "            print(f\"Very dodgy scan! Skipping\")\n",
    "            continue\n",
    "        print(f\"I have a DataSet {ds.dset} in sample {ds.sample}\")\n",
    "        ds.save()\n",
    "        \n",
    "        print(\"Segmenting\")\n",
    "        sbat = ImageD11.sinograms.lima_segmenter.setup(ds.dsfile, **seg_pars)\n",
    "        \n",
    "        if sbat is None:\n",
    "            print(f\"{dataset} in sample {sample} already segmented, skipping\")\n",
    "            continue\n",
    "        \n",
    "        utils.slurm_submit_and_wait(sbat, 60)\n",
    "        \n",
    "        print(\"Labelling sparse peaks\")\n",
    "        ImageD11.sinograms.assemble_label.main(ds.dsfile, ds.sparsefile)\n",
    "        \n",
    "        print(\"Generating peaks table\")\n",
    "        ImageD11.sinograms.properties.main(ds.dsfile, ds.sparsefile, ds.pksfile, options={'algorithm': 'lmlabel', 'wtmax': 70000, 'save_overlaps': False})\n",
    "        \n",
    "        print(\"Cleaning up sparse files\")\n",
    "        sparse_folder_path = os.path.join(ds.analysispath, \"sparse\")\n",
    "\n",
    "        if not os.path.exists(sparse_folder_path):\n",
    "            os.mkdir(sparse_folder_path)\n",
    "\n",
    "        scan_sparse_files = glob.glob(os.path.join(ds.analysispath, \"scan*_sparse.h5\"))\n",
    "\n",
    "        for scan_sparse_file in scan_sparse_files:\n",
    "            shutil.move(scan_sparse_file, sparse_folder_path)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee80e4-d426-46a9-b635-a28ded5039e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (main)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
